{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"__7M23AC7Jlf"},"outputs":[],"source":["# Imports\n","\n","import torch\n","import torch.utils.data\n","import torchvision\n","import pandas\n","import os\n","import typing\n","import random\n","import PIL.Image\n","import PIL.ImageDraw\n","import matplotlib.pyplot\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"elZKoYx37Jlj"},"outputs":[],"source":["# Constants\n","IMAGE_DIRECTORY_PATH = \"archive/license_plates_detection_train/\"\n","IMAGE_ANNOTATIONS_CSV_PATH = \"archive/license_plates_detection_train.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9AedDd1f7Jlj"},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, data : typing.Sequence[typing.Tuple[object, object]], input_transform = None, target_transforms=None):\n","        self.input_transform = input_transform\n","        self.target_transforms = target_transforms\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        input, target = self.data[index]\n","\n","        if self.input_transform is not None:\n","            input = self.input_transform(input)\n","\n","        if self.target_transforms is not None:\n","            target = self.target_transforms(target)\n","\n","        return input, target\n","\n","    def split(self, ratio):\n","        split_index = int(ratio * len(self.data))\n","        return (Dataset(self.data[split_index:], self.input_transform, self.target_transforms),\n","                Dataset(self.data[:split_index], self.input_transform, self.target_transforms))\n","\n","    def shuffle(self, seed = None):\n","        if seed is not None:\n","            random.seed(seed)\n","        shuffled_data = self.data\n","        random.shuffle(shuffled_data)\n","        return Dataset(shuffled_data, self.input_transform, self.target_transforms)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mw05MwE97Jll"},"outputs":[],"source":["annotations: typing.Sequence[object]\n","data: typing.Sequence[typing.Tuple[object, object]]\n","\n","with open(IMAGE_ANNOTATIONS_CSV_PATH, \"r\") as file:\n","    # split by row and comma\n","    lines = list(map(lambda x: x.split(\",\"), file.read().splitlines()))\n","    # zip column names together with values\n","    annotations = list(map(lambda x: dict(zip(lines[0], x)), lines[1:]))\n","    # convert all digit strings members of object to int\n","    annotations = list(map(lambda x: {k: int(v) if v.isdigit() else v for k, v in x.items()}, annotations))\n","\n","inputs = [PIL.Image.open(f\"{IMAGE_DIRECTORY_PATH}{annotation['img_id']}\") for annotation in annotations]\n","targets = [torch.Tensor([annotation['xmin'], annotation['ymin'], annotation['xmax'], annotation['ymax']]) for annotation in annotations]\n","data = list(zip(inputs, targets))\n","\n","MEAN = [0.485, 0.456, 0.406]\n","STD = [0.229, 0.224, 0.225]\n","\n","input_transform = torchvision.transforms.Compose(\n","    [\n","        torchvision.transforms.Resize(256),\n","        torchvision.transforms.CenterCrop(224),\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Normalize(MEAN, STD),\n","        #lambda x : x.unsqueeze(0)\n","    ]\n",")\n","\n","target_transform = torchvision.transforms.Compose(\n","    [\n","    ]\n",")\n","\n","dataset = Dataset(data, input_transform)  # , target_transform)\n","\n","training_dataset, validation_dataset = dataset.shuffle().split(0.8)\n","\n","training_dataloader, validation_dataloader = \\\n","    torch.utils.data.DataLoader(training_dataset, batch_size=1, shuffle=True),\\\n","    torch.utils.data.DataLoader(validation_dataset, batch_size=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojG87EoL7Jlm"},"outputs":[],"source":["class LicensePlateBBOXDetector(torch.nn.Module):\n","    def __init__(self):\n","        super(LicensePlateBBOXDetector, self).__init__()\n","        \n","        self.base_model = torchvision.models.resnet50(pretrained=True)\n","        self.base_model.eval()\n","\n","        self.net = torch.nn.Sequential(\n","            torch.nn.Linear(self.base_model.fc.in_features, 128),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(128, 64),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(64, 32),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(32, 4),\n","            torch.nn.Sigmoid()\n","        )\n","\n","        self.base_model.fc = torch.nn.Identity()\n","\n","    def forward(self, input):\n","        input = self.base_model(input)\n","        return input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vz40puvI7Jlm","outputId":"f1cb6087-8a4d-4f9c-aaef-40372604d75a"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\torch\\nn\\modules\\loss.py:96: UserWarning: Using a target size (torch.Size([1, 4])) that is different to the input size (torch.Size([1, 2048])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.l1_loss(input, target, reduction=self.reduction)\n"]},{"ename":"RuntimeError","evalue":"The size of tensor a (2048) must match the size of tensor b (4) at non-singleton dimension 1","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18560\\1379068738.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL1Loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;31m#train(1, training_dataloader, model, torch.nn.L1Loss(), optimizer)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18560\\1379068738.py\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(dataloader, model, criterion)\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36ml1_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3228\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3230\u001b[1;33m     \u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3231\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2048) must match the size of tensor b (4) at non-singleton dimension 1"]}],"source":["model = LicensePlateBBOXDetector()\n","criterion = torch.nn.L1Loss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","\n","def train(num_epochs, dataloader, model, criterion, optimizer):\n","    for epoch in range(num_epochs):\n","        for index, (inputs, targets) in enumerate(dataloader):\n","            if index % 100 == 0:\n","                print(f\"Processing batch: {index}/{len(dataloader)}\")\n","            for batch_index in range(len(inputs)):\n","                input, target = inputs[batch_index], targets[batch_index]\n","                input = input.unsqueeze(0)\n","                target = target.unsqueeze(0)\n","\n","                output = model(input)\n","                loss = criterion(output, target)\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","def validate(dataloader, model, criterion):\n","    total_loss = 0.0\n","    with torch.no_grad():\n","        for index, (inputs, targets) in enumerate(dataloader):\n","            for batch_index in range(len(inputs)):\n","                input, target = inputs[batch_index].unsqueeze(0), targets[batch_index].unsqueeze(0)\n","                output = model(input)\n","                loss = criterion(output, target)\n","                total_loss += loss\n","                break\n","\n","    print(f\"Total loss: {total_loss:.3f}\")\n","\n","\n","validate(validation_dataloader, model, torch.nn.L1Loss())\n","#train(1, training_dataloader, model, torch.nn.L1Loss(), optimizer)\n"]}],"metadata":{"interpreter":{"hash":"0a54084e6b208ee8d1ce3989ffc20924477a5f55f5a43e22e699a6741623861e"},"kernelspec":{"display_name":"Python 3.7.8 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"},"orig_nbformat":4,"colab":{"name":"license_plate_recognition_3_Christian.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}