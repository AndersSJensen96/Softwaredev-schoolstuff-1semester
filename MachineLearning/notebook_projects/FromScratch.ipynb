{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FromScratch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPj0VZr01d3kOCBRnWcFzbR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**DRIVE MOUNT**"],"metadata":{"id":"eXylZQz4BCQK"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aIRvhrLQlRIA","executionInfo":{"status":"ok","timestamp":1653303707837,"user_tz":-120,"elapsed":17992,"user":{"displayName":"Anders Skov Jensen","userId":"00655611242761101780"}},"outputId":"fea9571a-839a-4300-c2d2-fc7d17def2b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["**IMPORTS**"],"metadata":{"id":"FOQOu4BoA99R"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from skimage.io import imread\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision import models\n","\n","from torch.utils.data import Dataset\n","from torchvision.io import read_image\n","from torchvision.utils import save_image\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from PIL import Image\n","import pandas as pd\n","import os\n","\n","from torchvision.utils import draw_segmentation_masks\n","from torch.functional import Tensor\n","\n","import cv2\n","\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","\n","currentDirPath = \"./drive/MyDrive/Software udvikling/DeepLearning/Eksamen/\""],"metadata":{"id":"TjgmTOHrlZRr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**MASKING** (virker ikke, men tror det er tæt på)"],"metadata":{"id":"WOenKI_YA4ww"}},{"cell_type":"markdown","source":["**ARTICLE MASKING FUNCTIONS** (Got it to work)"],"metadata":{"id":"WM0Al4CUpnhB"}},{"cell_type":"code","source":["#https://towardsdatascience.com/bounding-box-prediction-from-scratch-using-pytorch-a8525da51ddc\n","#https://jovian.ai/aakanksha-ns/road-signs-bounding-box-prediction\n","def create_mask(bounding_box_coord, image):\n","    \"\"\"Creates a mask for the bounding box of same shape as image\"\"\"\n","    *_,rows,cols = image.shape\n","    mask = np.zeros((rows, cols))\n","    bb = bounding_box_coord.astype(np.int)\n","    mask[bb[0]:bb[2], bb[1]:bb[3]] = 1.\n","    return mask\n","\n","def mask_to_bb(mask):\n","    \"\"\"Convert mask Y to a bounding box, assumes 0 as background nonzero object\"\"\"\n","    cols, rows = np.nonzero(mask)\n","    if len(cols)==0: \n","        return np.zeros(4, dtype=np.float32)\n","    top_row = np.min(rows)\n","    left_col = np.min(cols)\n","    bottom_row = np.max(rows)\n","    right_col = np.max(cols)\n","    return np.array([left_col, top_row, right_col, bottom_row], dtype=np.float32)\n","\n","\n","def create_bb_array(csvRow):\n","    \"\"\"Generates bounding box array from a train_df row\"\"\"\n","    return np.array([csvRow['new_bbs_ymin'],csvRow['new_bbs_xmin'],csvRow['new_bbs_ymax'],csvRow['new_bbs_xmax']])\n"],"metadata":{"id":"dbxsA2Zaj8Rj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**VARIABLES FOR TEST**"],"metadata":{"id":"DhPdo03HAwNW"}},{"cell_type":"code","source":["#CSV PATH\n","csvPathTest = currentDirPath + \"TrainingData/D1/D1.csv\"\n","#Read CSV\n","csvTest = pd.read_csv(csvPathTest)\n","#PULL FIRST ROW\n","firstImageTest = csvTest.iloc[0]\n","#CONSTRUCT IMAGE PATH\n","imgPathTest = currentDirPath + \"TrainingData/D1/Images/\" + firstImageTest['img_id']\n","#READ IMAGE\n","imageTest = read_image(imgPathTest)"],"metadata":{"id":"f7HgEjCMAvmh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TESTING MAKS FUNCTIONS**"],"metadata":{"id":"GciWADI34RPg"}},{"cell_type":"code","source":["#RETRIEVE BB COORDS\n","bbTest = create_bb_array(firstImageTest)\n","#CREATE MASK\n","maskTest = create_mask(bbTest, imageTest)\n","#PRINT BB IMAGE IN PLOT\n","plt.imshow(maskTest, cmap='gray')"],"metadata":{"id":"a1Gr9IfY4Q38"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Save Mask**"],"metadata":{"id":"KrVo710KG0na"}},{"cell_type":"code","source":["#df_targets = pd.read_csv(currentDirPath + \"TrainingData/D1/D1.csv\")\n","df_targets = pd.read_csv(currentDirPath + \"TrainingData/D1/D1NewBBValue.csv\")\n","for index, row in df_targets.iterrows():\n","  #CONSTRUCT IMAGE PATH\n","  imgPath = currentDirPath + \"TrainingData/D1/Images/\" + row['img_id']\n","  #READ IMAGE\n","  image = read_image(imgPath)\n","  #RETRIEVE BB COORDS\n","  bb = create_bb_array(row)\n","  #CREATE MASK\n","  mask = create_mask(bb, image)\n","  mask = torch.tensor(mask, dtype=torch.float32)\n","  #print(mask)\n","  #SAVE MASK\n","  save_image(mask, currentDirPath + \"TrainingData/D1/Target_Images/\" + row['img_id'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"id":"BD9DC2xzGzRD","executionInfo":{"status":"error","timestamp":1653290882075,"user_tz":-120,"elapsed":994903,"user":{"displayName":"Anders Skov Jensen","userId":"00655611242761101780"}},"outputId":"5a4aad9a-b1c5-4a0d-f6e6-eabf7b1f603f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  import sys\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-92e2b3757ebc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mimgPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrentDirPath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"TrainingData/D1/Images/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m#READ IMAGE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;31m#RETRIEVE BB COORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mbb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_bb_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(path, mode)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mdecode_image\u001b[0;34m(input, mode)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["**RESIZE AND SAVE NEW BOUNDING BOX IMAGE FUNCTION**"],"metadata":{"id":"mlXncvhWztkz"}},{"cell_type":"code","source":["def resize_image_bb(read_path,write_path,row,size):\n","    \"\"\"Resize an image and its bounding box and write image to new path\"\"\"\n","    bounding_box = create_bb_array(row)\n","    img = read_image(read_path)\n","    transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize(size),\n","        transforms.ToTensor()\n","    ])\n","    im_resized = transform(img)\n","    #print(im_resized.shape)\n","    mask = create_mask(bounding_box, img)\n","    #print(mask.shape)\n","    Y_resized = cv2.resize(mask, size)\n","    #print(Y_resized)\n","    new_path = currentDirPath + write_path + row['img_id']\n","    #print(new_path)\n","    save_image(im_resized, new_path)\n","    return write_path + row['img_id'], mask_to_bb(Y_resized)"],"metadata":{"id":"iDBz5ExUyl_V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TEST REZISE METHOD**"],"metadata":{"id":"LsVJtIX5AYYO"}},{"cell_type":"code","source":["new_path,new_bb = resize_image_bb(imgPathTest, \"TrainingData/D1/Images_Resized_bb/\", firstImageTest,(200,300))\n","print(new_path, new_bb)    "],"metadata":{"id":"LSTtx47HAYKS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**GET DATA TO MODIFY**"],"metadata":{"id":"WjD1wPI81Dyp"}},{"cell_type":"code","source":["csv = currentDirPath + \"TrainingData/D1/D1.csv\"\n","imgDir = currentDirPath + \"TrainingData/D1/Images/\"\n","\n","df_train = pd.read_csv(csv)"],"metadata":{"id":"5dNYhKpQ1C2t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**LOOP THROUGH ALL IMAGES AND MAKE RESIZED BOUNDING BOX** (RUN ONLY IF YOU NEED TO CHANGE THE SIZES)"],"metadata":{"id":"5aL_c7sjzlvS"}},{"cell_type":"code","source":["new_paths = []\n","new_bbs_ymin = []\n","new_bbs_xmin = []\n","new_bbs_ymax = []\n","new_bbs_xmax = []\n","train_path_resized = \"TrainingData/D1/Images_Resized_bb/\"\n","count = 0\n","for index, row in df_train.iterrows():\n","    #print(row.values)\n","    #print(row['img_id'])\n","    read_path = imgDir + row['img_id']\n","    new_path,new_bb = resize_image_bb(read_path, train_path_resized, row,(200,300))\n","    new_paths.append(new_path)\n","    #new_bbs.append(new_bb)\n","    new_bbs_ymin.append(new_bb[0])\n","    new_bbs_xmin.append(new_bb[1])\n","    new_bbs_ymax.append(new_bb[2])\n","    new_bbs_xmax.append(new_bb[3])\n","df_train['new_path'] = new_paths\n","#df_train['new_bb'] = new_bbs\n","df_train['new_bbs_ymin'] = new_bbs_ymin\n","df_train['new_bbs_xmin'] = new_bbs_xmin\n","df_train['new_bbs_ymax'] = new_bbs_ymax\n","df_train['new_bbs_xmax'] = new_bbs_xmax"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":234},"id":"50m-Ls0dzd9c","executionInfo":{"status":"error","timestamp":1653289700682,"user_tz":-120,"elapsed":349,"user":{"displayName":"Anders Skov Jensen","userId":"00655611242761101780"}},"outputId":"bddc3b63-07f8-4bef-95fb-7b4b72ca1ba2"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-dd3bdb6e683f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_path_resized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"TrainingData/D1/Images_Resized_bb/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m#print(row.values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#print(row['img_id'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"]}]},{"cell_type":"markdown","source":["**SAVE NEW BOUNDING BOX VALUES** (RUN ONLY IF YOU'VE RUN THE PREVIOUS LOOP)"],"metadata":{"id":"cyLb3B5NMnp6"}},{"cell_type":"code","source":["csv_path_new_values = currentDirPath + \"TrainingData/D1/D1NewBBValue.csv\"\n","df_train.to_csv(csv_path_new_values)"],"metadata":{"id":"ZMDBbVpSMnOe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**LOAD CSV WITH BB VALUES**"],"metadata":{"id":"2Xz7pUMDOHXB"}},{"cell_type":"code","source":["csv_path_new_values = currentDirPath + \"TrainingData/D1/D1NewBBValue.csv\"\n","df_train = pd.read_csv(csv_path_new_values)"],"metadata":{"id":"B1t_X8syOFy0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**CHECK NEW VALUES IN DF_TRAIN**"],"metadata":{"id":"QYatKZ-Y4xdU"}},{"cell_type":"code","source":["print(df_train[['new_path', 'new_bbs_ymin']])"],"metadata":{"id":"hCU7z78g4wkF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**SPLIT SET INTO TRAINING AND VALIDATION**"],"metadata":{"id":"HiNkj55EzhOQ"}},{"cell_type":"code","source":["df_train = df_train\n","x_train, x_val = train_test_split(df_train[['img_id', 'ymin','xmin','ymax','xmax']], test_size=0.2, random_state=18)\n","\n","x_train = x_train.reset_index(drop=True)\n","x_val = x_val.reset_index(drop=True)"],"metadata":{"id":"nCs0c_mSyXxX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **DATASET**"],"metadata":{"id":"68Yev9brSsbj"}},{"cell_type":"code","source":["class CarsDataset(Dataset):\n","    def __init__(self, data, ActivateTransforms=False, width=200, height=300):\n","        self.transforms = ActivateTransforms\n","        self.data = data\n","        self.resizeWidth = width\n","        self.resizeHeight = height\n","        self.transformCompose = transforms.Compose([\n","              transforms.ToPILImage(),\n","              torchvision.transforms.Resize((self.resizeHeight,self.resizeWidth)),\n","              transforms.ToTensor(),\n","        ])\n","\n","    def __len__(self):\n","        return len(self.data['img_id'])\n","    \n","    def __getitem__(self, idx):\n","        #print(idx)\n","        imagePath = currentDirPath + \"TrainingData/D1/Images/\"+ self.data['img_id'][idx]\n","        #print(imagePath)\n","        image = read_image(imagePath)\n","        if(self.transforms):\n","          image = self.transformCompose(image)\n","        #print(self.data['new_bbs_ymin'][idx])\n","        bb = torch.Tensor([self.data['ymin'][idx]/self.resizeHeight,self.data['xmin'][idx]/self.resizeWidth,self.data['ymax'][idx]/self.resizeHeight,self.data['xmax'][idx]/self.resizeWidth])\n","        \n","        #bb = self.transformCompose2(bb)\n","        #print(bb)\n","        #print(type(image))\n","        #bb = torch.tensor(bb, dtype=torch.float32)\n","        #bb = bb.unsqueeze(-1)\n","        \n","        return image, bb "],"metadata":{"id":"4SSqtLlZR0TN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **DATALOADER**"],"metadata":{"id":"yXCnlYbyBKIS"}},{"cell_type":"code","source":["carsTrainSet = CarsDataset(x_train, ActivateTransforms=True)\n","carsValSet = CarsDataset(x_val, ActivateTransforms=False)\n","\n","carsTrainLoader = torch.utils.data.DataLoader(carsTrainSet, batch_size=4,\n","                                        shuffle=True, num_workers=2)\n","carsValLoader = torch.utils.data.DataLoader(carsValSet, batch_size=4,\n","                                         shuffle=True, num_workers=2)\n"],"metadata":{"id":"aWjz4DgBliw_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**MODEL**"],"metadata":{"id":"h6d9Mb3rBMJ4"}},{"cell_type":"markdown","source":["Resnet trænet til at tage input som billede i mini batches af 3 kanaler rgb (3 x H x W)\n","https://pytorch.org/hub/pytorch_vision_resnet/"],"metadata":{"id":"Atcgwux1xeRL"}},{"cell_type":"code","source":["class CarsModel(nn.Module):\n","    def __init__(self):\n","        super(CarsModel, self).__init__()        \n","        #resnet = models.resnet34(pretrained=True)\n","        #layers = list(resnet.children())[:8]\n","        # self.features1 = nn.Sequential(*layers[:6])\n","        # self.features2 = nn.Sequential(*layers[6:])\n","        # self.classifier = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 4))\n","        #self.bb = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 4))\n","        # self.featuresStart = nn.Linear(300, 1000)\n","        # self.features1 = nn.Linear(1000, 512)\n","        # self.features2 = nn.Linear(512, 256)\n","        # self.features3 = nn.Linear(256, 128)\n","        # self.features4 = nn.Linear(128, 64)\n","        # self.features5 = nn.Linear(64, 32)\n","        # self.features6 = nn.Linear(32, 16)\n","        # self.features7 = nn.Linear(16, 8)\n","        # self.features8 = nn.Linear(8,4)\n","        \n","        self.net = torch.nn.Sequential(\n","            torch.nn.Flatten(start_dim=0),\n","            torch.nn.Linear((200 * 300) * 3, 1000),\n","            torch.nn.Linear(1000, 500),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(500, 250),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(250, 4),\n","            torch.nn.Sigmoid()\n","        )\n","        \n","    def forward(self, x):\n","        # x = self.featuresStart(x)\n","        # x = self.features1(x)\n","        # x = self.features2(x)\n","        # x = self.features3(x)\n","        # x = self.features4(x)\n","        # x = self.features5(x)\n","        # x = self.features6(x)\n","        # x = self.features7(x)\n","        # x = self.features8(x)\n","        # x = F.relu(x)\n","        #x = nn.AdaptiveAvgPool2d((1,1))(x)\n","        #x = x.view(x.shape[0], -1)\n","        x = self.net(x)\n","        return x #self.classifier(x),self.bb(x)"],"metadata":{"id":"tkzWhkiTvPJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = CarsModel()\n","#Loss function \n","criterion = nn.MSELoss()#nn.CrossEntropyLoss()#F.binary_cross_entropy#nn.KLDivLoss()\n","#Optimizer\n","optimizer = torch.optim.SGD(net.parameters(), lr=0.01)"],"metadata":{"id":"kRB7iEv-vsRC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **TRAINING** "],"metadata":{"id":"ueLu5FXrBN7n"}},{"cell_type":"markdown","source":["Running as well as the other network, no clue why, no clue how, fuck this garbage.\n","Key error happens every now and again, no fucking clue either. prob the shitty split made earlier idk\n","https://towardsdatascience.com/creating-and-training-a-u-net-model-with-pytorch-for-2d-3d-semantic-segmentation-model-building-6ab09d6a0862"],"metadata":{"id":"T8CnqIdoxGdw"}},{"cell_type":"code","source":["running_loss = 0.0\n","for epoch in range(100):  # loop over the dataset multiple times\n","\n","    for i, data in enumerate(carsTrainLoader, 0):\n","\n","        inputs, labels = data\n","        #for batch_index in range(len(inputs)):\n","                #input, target = inputs[batch_index], labels[batch_index]\n","                #input = input.unsqueeze(0)\n","                #target = target.unsqueeze(0)\n","        #print(labels)\n","        #print(labels.shape)\n","        #print(inputs)\n","        #print(inputs.shape)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        #print(outputs.shape)\n","        #print(outputs)\n","        #print(labels)\n","        #print(classifier)\n","        #break\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","        print(\"Loss: \" + str(running_loss))\n","    print(\"Current Epoch:\" + epoch)\n","\n","print('Finished Training')"],"metadata":{"id":"7uN5x_Q9lqsi","colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"status":"error","timestamp":1653311496390,"user_tz":-120,"elapsed":3148,"user":{"displayName":"Anders Skov Jensen","userId":"00655611242761101780"}},"outputId":"75e75890-ccc3-4d6e-92ff-456f05c648d3"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-115-e26f45194baa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#print(inputs.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m#print(outputs.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#print(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-108-fe1b51b287aa>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m#x = nn.AdaptiveAvgPool2d((1,1))(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m#x = x.view(x.shape[0], -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;31m#self.classifier(x),self.bb(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3600x200 and 180000x1000)"]}]}]}